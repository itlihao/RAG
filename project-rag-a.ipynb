{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 项目实战：RAG企业知识库（上）\n",
    "\n",
    "## 课程目标\n",
    "通过本课程，学员将掌握基于检索增强生成（RAG）技术的智能问答系统的核心原理与实现，包括RAG架构设计、文档向量化、FastAPI后端开发、大模型集成及会话管理。\n",
    "\n",
    "## 课程内容\n",
    "1. RAG架构与基础知识回顾\n",
    "2. 向量嵌入与文档处理\n",
    "3. FastAPI后端开发\n",
    "4. 大模型接入与提示工程\n",
    "5. 数据持久化与会话管理\n",
    "\n",
    "## 前提条件\n",
    "- Python 3.9+\n",
    "- 安装必要的库：`fastapi`, `sentence-transformers`, `faiss-cpu`, `openai`, `sqlite3`, `uvicorn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAG架构与基础知识回顾\n",
    "\n",
    "### 1.1 RAG技术概述\n",
    "检索增强生成（Retrieval-Augmented Generation, RAG）是一种结合了信息检索和生成式模型的AI技术，核心原理是通过检索相关文档作为上下文，增强大语言模型的回答质量和准确性。RAG适用于知识密集型任务，如知识库问答、文档查询等。\n",
    "\n",
    "**核心流程**：\n",
    "- 用户提问 → 检索相关文档 → 构建上下文 → 语言模型生成回答\n",
    "\n",
    "<img src=\"./rag.png\" style=\"margin-left: 0px\" width=800px>\n",
    "\n",
    "\n",
    "**应用场景**：\n",
    "- 企业知识库问答\n",
    "- 学术研究辅助\n",
    "- 客户支持自动化\n",
    "\n",
    "### 1.2 向量数据库基础\n",
    "FAISS（Facebook AI Similarity Search）是一个高效的向量相似度搜索库，广泛用于RAG系统中存储和检索文档嵌入向量。\n",
    "\n",
    "**FAISS特性**：\n",
    "- 支持多种索引类型（如FlatL2、IVF、HNSW）\n",
    "- 高性能，适合大规模向量搜索\n",
    "- 易于与Python集成\n",
    "\n",
    "**代码示例**：初始化FAISS索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最近的向量索引： [[20 46 51]]\n",
      "距离： [[110.19388  111.065475 114.94035 ]]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 初始化FAISS索引\n",
    "dimension = 768  # 嵌入向量维度（m3e-base模型）\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# 假设有一些嵌入向量\n",
    "embeddings = np.random.random((100, dimension)).astype('float32')\n",
    "index.add(embeddings)  # 添加向量到索引\n",
    "\n",
    "# 搜索\n",
    "query_embedding = np.random.random((1, dimension)).astype('float32')\n",
    "k = 3  # 返回前3个最相似的向量\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "print(\"最近的向量索引：\", indices)\n",
    "print(\"距离：\", distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 语言模型集成\n",
    "RAG系统中，语言模型（如GLM-4-plus）负责根据检索到的上下文生成回答。通过API集成，可以轻松调用大模型。\n",
    "\n",
    "**关键点**：\n",
    "- 使用OpenAI兼容的API接口\n",
    "- 配置API密钥和端点\n",
    "- 确保上下文格式清晰\n",
    "\n",
    "**代码示例**：调用大语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG技术，全称为\"Retrieval-Augmented Generation\"（检索增强生成），是一种结合信息检索和生成模型的技术，广泛应用于自然语言处理（NLP）领域，特别是在问答系统、机器翻译和文本生成等任务中。\n",
      "\n",
      "### 核心思想\n",
      "RAG技术的核心思想是通过检索相关文档或知识库中的信息来增强生成模型的能力。具体来说，它包含两个主要组件：\n",
      "1. **检索模块**：负责从大量文档或知识库中检索与输入查询最相关的信息。\n",
      "2. **生成模块**：基于检索到的信息和原始查询生成最终输出。\n",
      "\n",
      "### 工作流程\n",
      "1. **输入查询**：用户或系统提供一个查询或问题。\n",
      "2. **信息检索**：检索模块从预定义的文档集合或知识库中找到与查询最相关的信息。\n",
      "3. **信息融合**：将检索到的信息与原始查询结合，提供给生成模块。\n",
      "4. **生成输出**：生成模块基于融合后的信息生成最终回答或文本。\n",
      "\n",
      "### 优势\n",
      "- **提高准确性**：通过引入外部知识，生成的回答或文本更准确、更有信息量。\n",
      "- **灵活性**：能够处理更广泛的问题类型，特别是那些需要特定领域知识的问题。\n",
      "- **可解释性**：生成的结果可以追溯到具体的文档或知识来源，增加了系统的透明度。\n",
      "\n",
      "### 应用场景\n",
      "- **问答系统**：如智能客服、搜索引擎的问答功能。\n",
      "- **机器翻译**：通过检索双语对齐的文档来提高翻译质量。\n",
      "- **文本生成**：如自动写作助手、新闻生成等。\n",
      "\n",
      "### 挑战\n",
      "- **检索效率**：在大规模文档集合中高效检索相关信息是一个挑战。\n",
      "- **信息融合**：如何有效融合检索到的信息和原始查询仍需深入研究。\n",
      "- **数据质量**：检索到的信息质量直接影响生成结果的质量。\n",
      "\n",
      "### 代表性模型\n",
      "- **DPR（Dense Passage Retrieval）**：一种高效的密集检索方法。\n",
      "- **T5（Text-to-Text Transfer Transformer）**：可以用于多种NLP任务的通用生成模型。\n",
      "\n",
      "RAG技术通过结合检索和生成，有效提升了NLP系统的性能和实用性，是当前自然语言处理领域的一个重要研究方向。\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 初始化OpenAI客户端\n",
    "client = OpenAI(\n",
    "    api_key=\"your api key\",\n",
    "    base_url=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "\n",
    "# 调用GLM-4-plus模型\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4-plus\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个专业的问答助手。\"},\n",
    "        {\"role\": \"user\", \"content\": \"什么是RAG技术？\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 向量嵌入与文档处理\n",
    "\n",
    "### 2.1 文档解析技术\n",
    "文档解析是将上传的文档（如TXT、PDF、DOCX）转换为纯文本，以便后续向量化处理。\n",
    "\n",
    "**关键点**：\n",
    "- 支持多种编码（如UTF-8、GBK）\n",
    "- 处理文件格式多样性\n",
    "- 确保内容提取的完整性\n",
    "\n",
    "**代码示例**：读取TXT文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "　　　　　　　　　　　　老子道德经\n",
      "\n",
      "\n",
      "　　　　　　　　　　　　　～　·※·　～\n",
      "\n",
      "一　　章　［道，可道，非恒道］　　　　　二　　章　［天下皆知美之为美］\n",
      "三　　章　［不尚贤］　　　　　　　　　　四\n"
     ]
    }
   ],
   "source": [
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, \"r\", encoding=\"gbk\", errors=\"ignore\") as f:\n",
    "            content = f.read()\n",
    "    return content\n",
    "\n",
    "# 示例\n",
    "content = read_text_file(\"docs/sample.txt\")\n",
    "print(content[:100])  # 打印前100个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 向量化模型选择\n",
    "SentenceTransformer（如m3e-base）用于将文本转换为固定维度的嵌入向量，适合语义检索。\n",
    "\n",
    "**m3e-base特性**：\n",
    "- 维度：768\n",
    "- 支持多语言\n",
    "- 高效且易于部署\n",
    "\n",
    "**代码示例**：生成嵌入向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install  sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 加载m3e-base模型\n",
    "model = SentenceTransformer('moka-ai/m3e-base')\n",
    "\n",
    "# 文本向量化\n",
    "texts = [\"这是一个示例文本\", \"另一个文档内容\"]\n",
    "embeddings = model.encode(texts, normalize_embeddings=True)\n",
    "print(\"嵌入向量形状：\", embeddings.shape)\n",
    "print(\"第一个向量：\", embeddings[0][:5])  # 打印前5个值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 文本分块策略\n",
    "将长文档分割为小块（chunks）以提高检索精度和效率。\n",
    "\n",
    "**策略**：\n",
    "- 固定长度分块\n",
    "- 按语义分块（如句子、段落）\n",
    "- 确保块之间有一定重叠\n",
    "\n",
    "**代码示例**：简单分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_length=500):\n",
    "    chunks = []\n",
    "    words = text.split()\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        current_length += len(word) + 1\n",
    "        if current_length >= max_length:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "# 示例\n",
    "text = \"这是一个很长的文档内容...\" * 100\n",
    "chunks = chunk_text(text)\n",
    "print(f\"分块数量：{len(chunks)}\")\n",
    "print(\"第一个分块：\", chunks[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**代码示例**：重叠分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文档分块函数\n",
    "def chunk_document(text, max_chars=500, overlap=100):\n",
    "    \"\"\"\n",
    "    将长文档切分成较小的块，使用滑动窗口确保上下文连贯性\n",
    "    \n",
    "    参数:\n",
    "        text: 要切分的文本\n",
    "        max_chars: 每个块的最大字符数\n",
    "        overlap: 相邻块之间的重叠字符数\n",
    "    \n",
    "    返回:\n",
    "        chunks: 切分后的文本块列表\n",
    "    \"\"\"\n",
    "    # 如果文本长度小于最大长度，直接返回\n",
    "    if len(text) <= max_chars:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    last_end = 0  # 跟踪上一次的结束位置\n",
    "    \n",
    "    while start < len(text):\n",
    "        # 确定当前块的结束位置\n",
    "        end = min(start + max_chars, len(text))\n",
    "        \n",
    "        # 如果没有到达文本末尾且不是最后一块，尝试在句子边界切分\n",
    "        if end < len(text):\n",
    "            # 在结束位置查找最近的句子结束标记\n",
    "            sentence_ends = [\n",
    "                m.end() for m in re.finditer(r'[。！？.!?]\\s*', text[start:end])\n",
    "            ]\n",
    "            \n",
    "            if sentence_ends:  # 如果找到句子结束标记，在最后一个句子结束处切分\n",
    "                end = start + sentence_ends[-1]\n",
    "            else:  # 如果没有找到，尝试在单词或标点处切分\n",
    "                last_space = text[start:end].rfind(' ')\n",
    "                last_punct = max(text[start:end].rfind('，'), text[start:end].rfind(','))\n",
    "                cut_point = max(last_space, last_punct)\n",
    "                \n",
    "                if cut_point > 0:  # 如果找到了合适的切分点\n",
    "                    end = start + cut_point + 1\n",
    "        \n",
    "        # 添加当前块到结果列表\n",
    "        chunks.append(text[start:end])\n",
    "        \n",
    "        # 检测是否有进展\n",
    "        if end <= last_end:\n",
    "            # 如果没有进展，强制向前移动，避免死循环\n",
    "            end = min(last_end + 1, len(text))\n",
    "            # 重新添加块，覆盖之前添加的\n",
    "            chunks[-1] = text[start:end]\n",
    "            \n",
    "            # 如果已到达文本末尾，退出循环\n",
    "            if end >= len(text):\n",
    "                break\n",
    "        \n",
    "        # 记录本次结束位置\n",
    "        last_end = end\n",
    "        \n",
    "        # 移动开始位置，考虑重叠\n",
    "        start = end - overlap\n",
    "        \n",
    "        # 确保开始位置不会后退\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "            \n",
    "        # 确保有进展，避免死循环\n",
    "        if start >= end:\n",
    "            start = end\n",
    "            \n",
    "        # 如果到达文本末尾，退出循环\n",
    "        if start >= len(text):\n",
    "            break\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FastAPI后端开发\n",
    "\n",
    "### 3.1 RESTful API设计\n",
    "FastAPI用于构建高效的异步API，支持文档上传、检索和问答功能。\n",
    "\n",
    "**关键API**：\n",
    "- `/api/upload`：上传文档\n",
    "- `/api/documents`：列出文档\n",
    "- `/api/stream`：处理问答请求\n",
    "\n",
    "**代码示例**：文档上传API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/api/upload\")\n",
    "async def upload_document(file: UploadFile = File(...)):\n",
    "    if not file.filename.endswith((\".txt\", \".pdf\", \".docx\")):\n",
    "        raise HTTPException(status_code=400, detail=\"仅支持.txt、.pdf、.docx文件\")\n",
    "    \n",
    "    os.makedirs(\"docs\", exist_ok=True)\n",
    "    file_path = os.path.join(\"docs\", file.filename)\n",
    "    content = await file.read()\n",
    "    \n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    doc_id = str(uuid.uuid4())\n",
    "    return {\"id\": doc_id, \"name\": file.filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 异步处理机制\n",
    "FastAPI的异步特性支持高并发处理，适合实时问答系统。\n",
    "\n",
    "**代码示例**：异步文档列表API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/api/documents\")\n",
    "async def list_documents():\n",
    "    documents = []\n",
    "    for filename in os.listdir(\"docs\"):\n",
    "        documents.append({\"id\": str(uuid.uuid4()), \"name\": filename})\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 流式响应实现\n",
    "通过Server-Sent Events（SSE）实现流式输出，提升用户体验。\n",
    "\n",
    "**code示例**：流式问答API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.responses import StreamingResponse\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "@app.get(\"/api/stream\")\n",
    "async def stream_response(query: str):\n",
    "    async def generate():\n",
    "        for i in range(5):\n",
    "            yield f\"data: {json.dumps({'content': f'回答部分 {i+1}', 'done': i == 4})}\n",
    "\n",
    "\"\n",
    "            await asyncio.sleep(1)\n",
    "    \n",
    "    return StreamingResponse(\n",
    "        generate(),\n",
    "        media_type=\"text/event-stream\",\n",
    "        headers={\n",
    "            \"Cache-Control\": \"no-cache\",\n",
    "            \"Connection\": \"keep-alive\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 大模型接入与提示工程\n",
    "\n",
    "### 4.1 大模型API集成\n",
    "通过OpenAI API集成GLM-4-plus模型，确保稳定性和高性能。\n",
    "\n",
    "**配置**：\n",
    "- API密钥：存储在环境变量或配置文件中\n",
    "- 端点：`https://open.bigmodel.cn/api/paas/v4/`\n",
    "\n",
    "**代码示例**：流式调用大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stream_llm_response(query: str):\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"glm-4-plus\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"你是一个专业的问答助手。\"},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    async def generate():\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                yield f\"data: {json.dumps({'content': chunk.choices[0].delta.content})}\n",
    "\n",
    "\"\n",
    "            if chunk.choices[0].finish_reason:\n",
    "                yield f\"data: {json.dumps({'done': True})}\n",
    "\n",
    "\"\n",
    "                break\n",
    "    \n",
    "    return StreamingResponse(generate(), media_type=\"text/event-stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 提示词设计\n",
    "有效的提示词可以显著提升模型输出质量。\n",
    "\n",
    "**设计原则**：\n",
    "- 明确指令：清楚描述任务\n",
    "- 提供上下文：包含检索到的文档内容\n",
    "- 限制输出：避免无关信息\n",
    "\n",
    "**示例提示**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "上下文信息:\n",
    "{context}\n",
    "\n",
    "问题: {query}\n",
    "请基于上下文信息回答问题，仅使用提供的信息，不要添加未提及的内容。\n",
    "\"\"\"\n",
    "\n",
    "# 示例使用\n",
    "context = \"相关文档：示例文档.txt\\n内容片段：RAG是一种结合检索和生成的AI技术。\"\n",
    "query = \"RAG技术是什么？\"\n",
    "formatted_prompt = prompt.format(context=context, query=query)\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 上下文管理\n",
    "优化上下文窗口大小，确保相关性并避免信息过载。\n",
    "\n",
    "**策略**：\n",
    "- 限制检索文档数量（如k=3）\n",
    "- 按相关性排序\n",
    "- 截断过长上下文\n",
    "\n",
    "**代码示例**：构建上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(retrieved_docs, retrieved_chunks):\n",
    "    context_parts = []\n",
    "    context_parts.append(\"相关文档:\\n\" + \"\\n\".join(retrieved_docs))\n",
    "    \n",
    "    if retrieved_chunks:\n",
    "        chunk_context = \"\\n\\n文档内容片段:\\n\"\n",
    "        for i, (doc_id, chunk) in enumerate(retrieved_chunks):\n",
    "            chunk_context += f\"[文档{i+1}] {chunk}\\n\"\n",
    "        context_parts.append(chunk_context)\n",
    "    else:\n",
    "        context_parts.append(\"\\n\\n没有找到相关的文档内容。\")\n",
    "    \n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "# 示例\n",
    "docs = [\"文档1.txt\", \"文档2.txt\"]\n",
    "chunks = [(\"doc1\", \"这是文档1的内容\"), (\"doc2\", \"这是文档2的内容\")]\n",
    "context = build_context(docs, chunks)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 数据持久化与会话管理\n",
    "\n",
    "### 5.1 SQLite数据库设计\n",
    "使用SQLite存储聊天历史和会话信息，轻量且易于部署。\n",
    "\n",
    "**表结构**：\n",
    "- `chat_sessions`：存储会话ID、摘要、时间戳\n",
    "- `messages`：存储消息内容、角色、时间戳\n",
    "\n",
    "**代码示例**：初始化数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def init_db():\n",
    "    conn = sqlite3.connect('chat_history.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS chat_sessions (\n",
    "        id TEXT PRIMARY KEY,\n",
    "        summary TEXT,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS messages (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        session_id TEXT,\n",
    "        role TEXT,\n",
    "        content TEXT,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        FOREIGN KEY (session_id) REFERENCES chat_sessions (id)\n",
    "    )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "init_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 会话状态管理\n",
    "支持会话创建、消息添加和历史恢复。\n",
    "\n",
    "**代码示例**：创建新会话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "async def create_new_chat_session(session_id, query, response):\n",
    "    summary = query[:30] + \"...\" if len(query) > 30 else query\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    conn = sqlite3.connect('chat_history.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO chat_sessions (id, summary, created_at, updated_at) VALUES (?, ?, ?, ?)\",\n",
    "        (session_id, summary, current_time, current_time)\n",
    "    )\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO messages (session_id, role, content, created_at) VALUES (?, ?, ?, ?)\",\n",
    "        (session_id, \"user\", query, current_time)\n",
    "    )\n",
    "    \n",
    "    cursor.execute(\n",
    "        \"INSERT INTO messages (session_id, role, content, created_at) VALUES (?, ?, ?, ?)\",\n",
    "        (session_id, \"bot\", response, current_time)\n",
    "    )\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# 示例\n",
    "session_id = str(uuid.uuid4())\n",
    "await create_new_chat_session(session_id, \"什么是RAG？\", \"RAG是一种结合检索和生成的AI技术。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结与实践\n",
    "\n",
    "### 关键点回顾\n",
    "- **RAG架构**：结合检索和生成，提升回答质量\n",
    "- **向量嵌入**：使用m3e-base模型将文档转为向量\n",
    "- **FastAPI**：实现高效的异步API和流式响应\n",
    "- **大模型**：通过API集成大模型，优化提示词\n",
    "- **会话管理**：使用SQLite存储历史记录\n",
    "\n",
    "### 实践任务\n",
    "1. 初始化一个简单的RAG系统，包含文档上传和检索功能\n",
    "2. 实现一个流式问答接口，集成m3e-base和大模型API\n",
    "3. 添加会话管理功能，支持历史记录保存和恢复\n",
    "\n",
    "### 下一步\n",
    "- 学习前端开发，构建响应式界面\n",
    "- 集成语音交互功能\n",
    "- 系统优化和功能拓展\n",
    "- 系统部署和运行"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
